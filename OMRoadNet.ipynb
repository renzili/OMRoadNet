{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VIGyIus8Vr7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wNjDKdQy35h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRm-USlsHgEV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt3igws3eiVp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1EySlOXwwoa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8daqlgVhw29P",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Download one of the official datasets with:\n",
    "\n",
    "-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos]`\n",
    "\n",
    "Or use your own dataset by creating the appropriate folders and adding in the images.\n",
    "\n",
    "-   Create a dataset folder under `/dataset` for your dataset.\n",
    "-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrdOettJxaCc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bash' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!bash ./datasets/download_cyclegan_dataset.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdUz4116xhpm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pretrained models\n",
    "\n",
    "Download one of the official pretrained models with:\n",
    "\n",
    "-   `bash ./scripts/download_cyclegan_model.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n",
    "\n",
    "Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B75UqtKhxznS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!bash ./scripts/download_cyclegan_model.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFw1kDQBx3LN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\n",
    "\n",
    "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n",
    "\n",
    "Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n",
    "\n",
    "Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sp7TCT2x9dB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan --display_id -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UkcaFZiyASl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing\n",
    "\n",
    "-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\n",
    "\n",
    "Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n",
    "\n",
    "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
    "> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n",
    "\n",
    "> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCsKkEq0yGh0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzSKIPUByfiN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Mgg8raPyizq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G3oVH9DyqLQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: ./datasets/maps               \t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: 1                             \n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: maps_cyclegan                 \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 1096\n",
      "initialize network with normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.378 M\n",
      "[Network G_B] Total number of parameters : 11.378 M\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\ANACONDA\\envs\\cyclegan\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create web directory ./checkpoints\\maps_cyclegan\\web...\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 1, iters: 100, time: 0.723, data: 10.578) D_A: 0.331 G_A: 0.402 cycle_A: 2.589 idt_A: 0.446 D_B: 0.714 G_B: 0.862 cycle_B: 1.001 idt_B: 1.086 \n",
      "(epoch: 1, iters: 200, time: 0.683, data: 0.001) D_A: 0.270 G_A: 0.282 cycle_A: 1.850 idt_A: 0.324 D_B: 0.308 G_B: 0.361 cycle_B: 0.680 idt_B: 0.852 \n",
      "(epoch: 1, iters: 300, time: 0.686, data: 0.000) D_A: 0.370 G_A: 0.542 cycle_A: 2.452 idt_A: 0.374 D_B: 0.253 G_B: 0.323 cycle_B: 0.810 idt_B: 1.340 \n",
      "(epoch: 1, iters: 400, time: 0.925, data: 0.000) D_A: 0.314 G_A: 0.517 cycle_A: 1.949 idt_A: 0.607 D_B: 0.326 G_B: 0.337 cycle_B: 1.643 idt_B: 0.930 \n",
      "(epoch: 1, iters: 500, time: 0.730, data: 0.000) D_A: 0.353 G_A: 0.638 cycle_A: 1.771 idt_A: 0.292 D_B: 0.274 G_B: 0.254 cycle_B: 0.643 idt_B: 0.820 \n",
      "(epoch: 1, iters: 600, time: 0.685, data: 0.001) D_A: 0.200 G_A: 0.316 cycle_A: 1.327 idt_A: 1.053 D_B: 0.168 G_B: 0.550 cycle_B: 1.755 idt_B: 0.562 \n",
      "(epoch: 1, iters: 700, time: 0.688, data: 0.001) D_A: 0.146 G_A: 0.455 cycle_A: 1.555 idt_A: 0.768 D_B: 0.180 G_B: 0.306 cycle_B: 1.662 idt_B: 0.652 \n",
      "(epoch: 1, iters: 800, time: 0.782, data: 0.000) D_A: 0.194 G_A: 0.263 cycle_A: 1.722 idt_A: 0.728 D_B: 0.187 G_B: 0.542 cycle_B: 1.592 idt_B: 0.718 \n",
      "(epoch: 1, iters: 900, time: 0.684, data: 0.001) D_A: 0.222 G_A: 0.511 cycle_A: 1.310 idt_A: 0.493 D_B: 0.266 G_B: 0.367 cycle_B: 0.976 idt_B: 0.824 \n",
      "(epoch: 1, iters: 1000, time: 0.709, data: 0.001) D_A: 0.245 G_A: 0.394 cycle_A: 1.144 idt_A: 0.490 D_B: 0.184 G_B: 0.424 cycle_B: 0.735 idt_B: 0.575 \n",
      "End of epoch 1 / 200 \t Time Taken: 431 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 2, iters: 4, time: 0.695, data: 0.000) D_A: 0.231 G_A: 0.212 cycle_A: 1.363 idt_A: 0.228 D_B: 0.264 G_B: 0.285 cycle_B: 0.520 idt_B: 0.561 \n",
      "(epoch: 2, iters: 104, time: 0.962, data: 0.000) D_A: 0.192 G_A: 0.470 cycle_A: 1.881 idt_A: 0.265 D_B: 0.291 G_B: 0.144 cycle_B: 0.735 idt_B: 0.897 \n",
      "(epoch: 2, iters: 204, time: 0.715, data: 0.000) D_A: 0.075 G_A: 0.656 cycle_A: 0.954 idt_A: 0.384 D_B: 0.246 G_B: 0.426 cycle_B: 0.816 idt_B: 0.455 \n",
      "(epoch: 2, iters: 304, time: 0.716, data: 0.001) D_A: 0.048 G_A: 0.750 cycle_A: 1.285 idt_A: 0.573 D_B: 0.114 G_B: 1.011 cycle_B: 1.121 idt_B: 0.667 \n",
      "(epoch: 2, iters: 404, time: 0.700, data: 0.000) D_A: 0.209 G_A: 0.560 cycle_A: 3.153 idt_A: 0.409 D_B: 0.181 G_B: 0.214 cycle_B: 1.009 idt_B: 1.420 \n",
      "(epoch: 2, iters: 504, time: 0.807, data: 0.000) D_A: 0.441 G_A: 0.846 cycle_A: 0.982 idt_A: 0.433 D_B: 0.299 G_B: 0.710 cycle_B: 0.836 idt_B: 0.475 \n",
      "(epoch: 2, iters: 604, time: 0.687, data: 0.000) D_A: 0.203 G_A: 0.141 cycle_A: 1.015 idt_A: 0.376 D_B: 0.125 G_B: 0.556 cycle_B: 0.823 idt_B: 0.446 \n",
      "(epoch: 2, iters: 704, time: 0.700, data: 0.000) D_A: 0.360 G_A: 0.154 cycle_A: 1.409 idt_A: 0.336 D_B: 0.327 G_B: 0.300 cycle_B: 0.578 idt_B: 0.640 \n",
      "(epoch: 2, iters: 804, time: 0.743, data: 0.001) D_A: 0.123 G_A: 0.871 cycle_A: 1.942 idt_A: 0.486 D_B: 0.208 G_B: 0.731 cycle_B: 1.129 idt_B: 0.941 \n",
      "(epoch: 2, iters: 904, time: 0.982, data: 0.000) D_A: 0.185 G_A: 0.447 cycle_A: 1.913 idt_A: 0.302 D_B: 0.101 G_B: 0.283 cycle_B: 0.703 idt_B: 1.031 \n",
      "(epoch: 2, iters: 1004, time: 0.719, data: 0.000) D_A: 0.174 G_A: 0.341 cycle_A: 1.520 idt_A: 0.830 D_B: 0.203 G_B: 0.424 cycle_B: 1.767 idt_B: 0.755 \n",
      "End of epoch 2 / 200 \t Time Taken: 437 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 3, iters: 8, time: 0.695, data: 0.001) D_A: 0.220 G_A: 1.441 cycle_A: 1.881 idt_A: 0.726 D_B: 0.112 G_B: 0.946 cycle_B: 1.355 idt_B: 0.925 \n",
      "(epoch: 3, iters: 108, time: 0.727, data: 0.000) D_A: 0.167 G_A: 0.484 cycle_A: 1.124 idt_A: 0.224 D_B: 0.134 G_B: 0.405 cycle_B: 0.658 idt_B: 0.429 \n",
      "(epoch: 3, iters: 208, time: 0.976, data: 0.001) D_A: 0.123 G_A: 0.394 cycle_A: 1.369 idt_A: 0.176 D_B: 0.200 G_B: 0.598 cycle_B: 0.442 idt_B: 0.554 \n",
      "(epoch: 3, iters: 308, time: 0.700, data: 0.001) D_A: 0.229 G_A: 0.404 cycle_A: 1.153 idt_A: 0.602 D_B: 0.356 G_B: 0.086 cycle_B: 1.755 idt_B: 0.570 \n",
      "(epoch: 3, iters: 408, time: 0.696, data: 0.001) D_A: 0.147 G_A: 0.383 cycle_A: 1.398 idt_A: 0.542 D_B: 0.137 G_B: 0.468 cycle_B: 1.443 idt_B: 0.653 \n",
      "(epoch: 3, iters: 508, time: 0.691, data: 0.001) D_A: 0.115 G_A: 0.323 cycle_A: 1.177 idt_A: 0.219 D_B: 0.313 G_B: 0.208 cycle_B: 0.500 idt_B: 0.510 \n",
      "(epoch: 3, iters: 608, time: 0.807, data: 0.000) D_A: 0.235 G_A: 0.336 cycle_A: 1.374 idt_A: 0.489 D_B: 0.066 G_B: 0.696 cycle_B: 1.435 idt_B: 0.872 \n",
      "(epoch: 3, iters: 708, time: 0.693, data: 0.000) D_A: 0.163 G_A: 0.565 cycle_A: 1.603 idt_A: 0.232 D_B: 0.204 G_B: 0.375 cycle_B: 0.499 idt_B: 0.535 \n",
      "(epoch: 3, iters: 808, time: 0.716, data: 0.000) D_A: 0.093 G_A: 0.839 cycle_A: 1.337 idt_A: 0.329 D_B: 0.179 G_B: 0.280 cycle_B: 0.854 idt_B: 0.664 \n",
      "(epoch: 3, iters: 908, time: 0.697, data: 0.001) D_A: 0.330 G_A: 0.158 cycle_A: 0.924 idt_A: 0.225 D_B: 0.180 G_B: 0.241 cycle_B: 1.590 idt_B: 0.424 \n",
      "(epoch: 3, iters: 1008, time: 0.818, data: 0.000) D_A: 0.062 G_A: 0.729 cycle_A: 1.501 idt_A: 1.617 D_B: 0.126 G_B: 0.368 cycle_B: 3.047 idt_B: 0.936 \n",
      "End of epoch 3 / 200 \t Time Taken: 436 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 4, iters: 12, time: 0.731, data: 0.000) D_A: 0.145 G_A: 0.386 cycle_A: 0.725 idt_A: 0.357 D_B: 0.280 G_B: 0.292 cycle_B: 0.811 idt_B: 0.292 \n",
      "(epoch: 4, iters: 112, time: 0.729, data: 0.001) D_A: 0.155 G_A: 0.459 cycle_A: 1.025 idt_A: 0.345 D_B: 0.120 G_B: 0.203 cycle_B: 0.916 idt_B: 0.450 \n",
      "(epoch: 4, iters: 212, time: 0.713, data: 0.000) D_A: 0.171 G_A: 0.445 cycle_A: 1.271 idt_A: 0.142 D_B: 0.123 G_B: 0.104 cycle_B: 0.347 idt_B: 0.564 \n",
      "(epoch: 4, iters: 312, time: 0.976, data: 0.001) D_A: 0.184 G_A: 0.850 cycle_A: 0.989 idt_A: 0.210 D_B: 0.280 G_B: 0.470 cycle_B: 0.447 idt_B: 0.393 \n",
      "(epoch: 4, iters: 412, time: 0.695, data: 0.000) D_A: 0.083 G_A: 0.523 cycle_A: 0.845 idt_A: 0.149 D_B: 0.256 G_B: 0.686 cycle_B: 0.308 idt_B: 0.375 \n",
      "(epoch: 4, iters: 512, time: 0.693, data: 0.001) D_A: 0.098 G_A: 0.410 cycle_A: 1.949 idt_A: 0.167 D_B: 0.233 G_B: 0.336 cycle_B: 0.366 idt_B: 1.066 \n",
      "(epoch: 4, iters: 612, time: 0.690, data: 0.001) D_A: 0.121 G_A: 0.504 cycle_A: 1.500 idt_A: 0.198 D_B: 0.228 G_B: 0.235 cycle_B: 0.570 idt_B: 0.686 \n",
      "(epoch: 4, iters: 712, time: 0.915, data: 0.001) D_A: 0.083 G_A: 0.547 cycle_A: 2.030 idt_A: 0.242 D_B: 0.171 G_B: 0.268 cycle_B: 0.737 idt_B: 0.768 \n",
      "(epoch: 4, iters: 812, time: 0.698, data: 0.000) D_A: 0.141 G_A: 0.458 cycle_A: 1.044 idt_A: 0.171 D_B: 0.149 G_B: 0.278 cycle_B: 0.410 idt_B: 0.476 \n",
      "(epoch: 4, iters: 912, time: 0.708, data: 0.001) D_A: 0.058 G_A: 0.600 cycle_A: 1.510 idt_A: 0.489 D_B: 0.185 G_B: 0.235 cycle_B: 0.814 idt_B: 0.794 \n",
      "(epoch: 4, iters: 1012, time: 0.691, data: 0.000) D_A: 0.246 G_A: 0.459 cycle_A: 1.072 idt_A: 0.474 D_B: 0.185 G_B: 0.259 cycle_B: 1.170 idt_B: 0.466 \n",
      "End of epoch 4 / 200 \t Time Taken: 432 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 5, iters: 16, time: 1.004, data: 0.000) D_A: 0.213 G_A: 0.235 cycle_A: 1.129 idt_A: 0.179 D_B: 0.377 G_B: 0.059 cycle_B: 0.348 idt_B: 0.627 \n",
      "(epoch: 5, iters: 116, time: 0.692, data: 0.000) D_A: 0.074 G_A: 0.504 cycle_A: 0.946 idt_A: 0.731 D_B: 0.102 G_B: 0.298 cycle_B: 1.568 idt_B: 0.405 \n",
      "(epoch: 5, iters: 216, time: 0.699, data: 0.000) D_A: 0.196 G_A: 0.767 cycle_A: 1.554 idt_A: 0.508 D_B: 0.162 G_B: 1.075 cycle_B: 1.885 idt_B: 0.689 \n",
      "(epoch: 5, iters: 316, time: 0.689, data: 0.000) D_A: 0.171 G_A: 0.467 cycle_A: 1.022 idt_A: 0.404 D_B: 0.096 G_B: 0.218 cycle_B: 1.061 idt_B: 0.422 \n",
      "(epoch: 5, iters: 416, time: 0.826, data: 0.001) D_A: 0.113 G_A: 0.722 cycle_A: 0.895 idt_A: 0.271 D_B: 0.194 G_B: 0.273 cycle_B: 0.780 idt_B: 0.399 \n",
      "(epoch: 5, iters: 516, time: 0.692, data: 0.001) D_A: 0.071 G_A: 0.611 cycle_A: 1.053 idt_A: 0.291 D_B: 0.199 G_B: 0.268 cycle_B: 0.852 idt_B: 0.607 \n",
      "(epoch: 5, iters: 616, time: 0.696, data: 0.000) D_A: 0.165 G_A: 1.214 cycle_A: 1.219 idt_A: 0.180 D_B: 0.510 G_B: 0.466 cycle_B: 0.367 idt_B: 0.729 \n",
      "saving the latest model (epoch 5, total_iters 5000)\n",
      "(epoch: 5, iters: 716, time: 0.703, data: 0.001) D_A: 0.159 G_A: 0.284 cycle_A: 1.080 idt_A: 0.199 D_B: 0.061 G_B: 0.684 cycle_B: 0.629 idt_B: 0.455 \n",
      "(epoch: 5, iters: 816, time: 0.864, data: 0.001) D_A: 0.252 G_A: 0.314 cycle_A: 0.711 idt_A: 0.164 D_B: 0.525 G_B: 0.427 cycle_B: 0.366 idt_B: 0.422 \n",
      "(epoch: 5, iters: 916, time: 0.710, data: 0.000) D_A: 0.187 G_A: 0.430 cycle_A: 0.831 idt_A: 0.171 D_B: 0.068 G_B: 0.587 cycle_B: 0.586 idt_B: 0.674 \n",
      "(epoch: 5, iters: 1016, time: 0.704, data: 0.001) D_A: 0.119 G_A: 0.385 cycle_A: 1.203 idt_A: 0.172 D_B: 0.205 G_B: 0.439 cycle_B: 0.356 idt_B: 0.768 \n",
      "saving the model at the end of epoch 5, iters 5480\n",
      "End of epoch 5 / 200 \t Time Taken: 435 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 6, iters: 20, time: 0.696, data: 0.000) D_A: 0.549 G_A: 0.052 cycle_A: 0.994 idt_A: 0.317 D_B: 0.162 G_B: 0.716 cycle_B: 0.594 idt_B: 0.500 \n",
      "(epoch: 6, iters: 120, time: 0.950, data: 0.000) D_A: 0.132 G_A: 0.266 cycle_A: 1.671 idt_A: 0.238 D_B: 0.211 G_B: 0.710 cycle_B: 0.441 idt_B: 0.437 \n",
      "(epoch: 6, iters: 220, time: 0.693, data: 0.001) D_A: 0.222 G_A: 0.572 cycle_A: 0.927 idt_A: 0.155 D_B: 0.263 G_B: 0.264 cycle_B: 0.352 idt_B: 0.420 \n",
      "(epoch: 6, iters: 320, time: 0.720, data: 0.001) D_A: 0.114 G_A: 0.903 cycle_A: 1.040 idt_A: 0.228 D_B: 0.126 G_B: 0.347 cycle_B: 0.685 idt_B: 0.460 \n",
      "(epoch: 6, iters: 420, time: 0.702, data: 0.000) D_A: 0.249 G_A: 0.730 cycle_A: 1.724 idt_A: 0.212 D_B: 0.209 G_B: 0.217 cycle_B: 0.618 idt_B: 0.657 \n",
      "(epoch: 6, iters: 520, time: 0.986, data: 0.001) D_A: 0.168 G_A: 0.306 cycle_A: 0.891 idt_A: 0.133 D_B: 0.133 G_B: 0.244 cycle_B: 0.294 idt_B: 0.364 \n",
      "(epoch: 6, iters: 620, time: 0.719, data: 0.001) D_A: 0.069 G_A: 0.568 cycle_A: 1.322 idt_A: 0.221 D_B: 0.171 G_B: 0.516 cycle_B: 0.434 idt_B: 0.579 \n",
      "(epoch: 6, iters: 720, time: 0.715, data: 0.000) D_A: 0.138 G_A: 1.101 cycle_A: 0.773 idt_A: 0.259 D_B: 0.193 G_B: 0.796 cycle_B: 0.681 idt_B: 0.372 \n",
      "(epoch: 6, iters: 820, time: 0.700, data: 0.000) D_A: 0.197 G_A: 0.353 cycle_A: 0.574 idt_A: 0.128 D_B: 0.228 G_B: 0.272 cycle_B: 0.378 idt_B: 0.237 \n",
      "(epoch: 6, iters: 920, time: 0.837, data: 0.000) D_A: 0.182 G_A: 0.460 cycle_A: 1.102 idt_A: 0.296 D_B: 0.106 G_B: 0.447 cycle_B: 0.579 idt_B: 0.448 \n",
      "(epoch: 6, iters: 1020, time: 0.697, data: 0.001) D_A: 0.109 G_A: 0.822 cycle_A: 1.333 idt_A: 0.192 D_B: 0.248 G_B: 0.302 cycle_B: 0.432 idt_B: 0.471 \n",
      "End of epoch 6 / 200 \t Time Taken: 434 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 7, iters: 24, time: 0.733, data: 0.001) D_A: 0.123 G_A: 0.555 cycle_A: 1.655 idt_A: 0.137 D_B: 0.111 G_B: 0.666 cycle_B: 0.331 idt_B: 0.732 \n",
      "(epoch: 7, iters: 124, time: 0.695, data: 0.000) D_A: 0.457 G_A: 0.509 cycle_A: 1.564 idt_A: 0.364 D_B: 0.081 G_B: 0.349 cycle_B: 0.741 idt_B: 0.620 \n",
      "(epoch: 7, iters: 224, time: 1.029, data: 0.001) D_A: 0.215 G_A: 0.497 cycle_A: 0.968 idt_A: 0.121 D_B: 0.300 G_B: 0.089 cycle_B: 0.300 idt_B: 0.410 \n",
      "(epoch: 7, iters: 324, time: 0.703, data: 0.001) D_A: 0.126 G_A: 0.671 cycle_A: 1.227 idt_A: 0.141 D_B: 0.241 G_B: 0.180 cycle_B: 0.362 idt_B: 0.579 \n",
      "(epoch: 7, iters: 424, time: 0.743, data: 0.001) D_A: 0.098 G_A: 0.466 cycle_A: 1.054 idt_A: 0.156 D_B: 0.056 G_B: 0.283 cycle_B: 0.385 idt_B: 0.465 \n",
      "(epoch: 7, iters: 524, time: 0.746, data: 0.000) D_A: 0.251 G_A: 0.720 cycle_A: 1.339 idt_A: 0.166 D_B: 0.098 G_B: 0.259 cycle_B: 0.500 idt_B: 0.445 \n",
      "(epoch: 7, iters: 624, time: 0.871, data: 0.001) D_A: 0.218 G_A: 0.567 cycle_A: 1.127 idt_A: 0.119 D_B: 0.031 G_B: 0.063 cycle_B: 0.334 idt_B: 0.498 \n",
      "(epoch: 7, iters: 724, time: 0.694, data: 0.000) D_A: 0.093 G_A: 0.548 cycle_A: 0.694 idt_A: 0.305 D_B: 0.290 G_B: 0.386 cycle_B: 0.769 idt_B: 0.302 \n",
      "(epoch: 7, iters: 824, time: 0.699, data: 0.000) D_A: 0.416 G_A: 0.115 cycle_A: 1.007 idt_A: 0.197 D_B: 0.090 G_B: 0.373 cycle_B: 0.450 idt_B: 0.397 \n",
      "(epoch: 7, iters: 924, time: 0.695, data: 0.000) D_A: 0.217 G_A: 0.373 cycle_A: 1.323 idt_A: 0.111 D_B: 0.102 G_B: 0.676 cycle_B: 0.272 idt_B: 0.636 \n",
      "(epoch: 7, iters: 1024, time: 0.866, data: 0.000) D_A: 0.208 G_A: 0.490 cycle_A: 1.314 idt_A: 0.747 D_B: 0.202 G_B: 0.354 cycle_B: 1.599 idt_B: 0.363 \n",
      "End of epoch 7 / 200 \t Time Taken: 440 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 8, iters: 28, time: 0.698, data: 0.001) D_A: 0.280 G_A: 0.119 cycle_A: 1.055 idt_A: 0.131 D_B: 0.224 G_B: 0.406 cycle_B: 0.309 idt_B: 0.472 \n",
      "(epoch: 8, iters: 128, time: 0.735, data: 0.000) D_A: 0.211 G_A: 0.495 cycle_A: 0.635 idt_A: 0.171 D_B: 0.080 G_B: 0.675 cycle_B: 0.333 idt_B: 0.500 \n",
      "(epoch: 8, iters: 228, time: 0.703, data: 0.000) D_A: 0.059 G_A: 0.844 cycle_A: 1.472 idt_A: 0.151 D_B: 0.232 G_B: 0.317 cycle_B: 0.385 idt_B: 0.694 \n",
      "(epoch: 8, iters: 328, time: 1.032, data: 0.000) D_A: 0.276 G_A: 0.208 cycle_A: 1.006 idt_A: 0.142 D_B: 0.028 G_B: 0.265 cycle_B: 0.281 idt_B: 0.424 \n",
      "(epoch: 8, iters: 428, time: 0.702, data: 0.000) D_A: 0.227 G_A: 0.550 cycle_A: 1.398 idt_A: 0.111 D_B: 0.217 G_B: 0.199 cycle_B: 0.255 idt_B: 0.824 \n",
      "(epoch: 8, iters: 528, time: 0.700, data: 0.000) D_A: 0.213 G_A: 0.268 cycle_A: 1.413 idt_A: 0.111 D_B: 0.094 G_B: 0.686 cycle_B: 0.339 idt_B: 0.607 \n",
      "(epoch: 8, iters: 628, time: 0.695, data: 0.000) D_A: 0.086 G_A: 0.348 cycle_A: 0.969 idt_A: 0.211 D_B: 0.111 G_B: 0.825 cycle_B: 0.769 idt_B: 0.400 \n",
      "(epoch: 8, iters: 728, time: 0.850, data: 0.000) D_A: 0.145 G_A: 0.490 cycle_A: 1.248 idt_A: 0.122 D_B: 0.086 G_B: 0.536 cycle_B: 0.425 idt_B: 0.540 \n",
      "(epoch: 8, iters: 828, time: 0.706, data: 0.000) D_A: 0.059 G_A: 0.324 cycle_A: 0.796 idt_A: 0.158 D_B: 0.183 G_B: 0.061 cycle_B: 0.331 idt_B: 0.380 \n",
      "(epoch: 8, iters: 928, time: 0.712, data: 0.000) D_A: 0.067 G_A: 0.613 cycle_A: 0.980 idt_A: 0.219 D_B: 0.180 G_B: 0.977 cycle_B: 0.558 idt_B: 0.441 \n",
      "(epoch: 8, iters: 1028, time: 0.706, data: 0.000) D_A: 0.073 G_A: 0.581 cycle_A: 1.306 idt_A: 0.148 D_B: 0.072 G_B: 1.089 cycle_B: 0.331 idt_B: 0.468 \n",
      "End of epoch 8 / 200 \t Time Taken: 437 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 9, iters: 32, time: 0.984, data: 0.000) D_A: 0.101 G_A: 0.412 cycle_A: 1.026 idt_A: 0.092 D_B: 0.063 G_B: 0.325 cycle_B: 0.228 idt_B: 0.432 \n",
      "(epoch: 9, iters: 132, time: 0.700, data: 0.000) D_A: 0.284 G_A: 0.464 cycle_A: 1.175 idt_A: 0.411 D_B: 0.147 G_B: 0.499 cycle_B: 0.586 idt_B: 0.691 \n",
      "(epoch: 9, iters: 232, time: 0.701, data: 0.001) D_A: 0.389 G_A: 0.694 cycle_A: 1.143 idt_A: 0.178 D_B: 0.310 G_B: 0.081 cycle_B: 0.414 idt_B: 0.435 \n",
      "(epoch: 9, iters: 332, time: 0.694, data: 0.000) D_A: 0.095 G_A: 0.839 cycle_A: 1.131 idt_A: 0.155 D_B: 0.169 G_B: 0.430 cycle_B: 0.270 idt_B: 0.526 \n",
      "(epoch: 9, iters: 432, time: 0.845, data: 0.001) D_A: 0.132 G_A: 0.489 cycle_A: 1.046 idt_A: 0.145 D_B: 0.367 G_B: 0.057 cycle_B: 0.416 idt_B: 0.338 \n",
      "(epoch: 9, iters: 532, time: 0.704, data: 0.000) D_A: 0.120 G_A: 0.378 cycle_A: 1.142 idt_A: 0.161 D_B: 0.094 G_B: 0.455 cycle_B: 0.466 idt_B: 0.390 \n",
      "(epoch: 9, iters: 632, time: 0.688, data: 0.000) D_A: 0.192 G_A: 0.196 cycle_A: 1.105 idt_A: 0.130 D_B: 0.123 G_B: 0.592 cycle_B: 0.275 idt_B: 0.419 \n",
      "(epoch: 9, iters: 732, time: 0.687, data: 0.001) D_A: 0.122 G_A: 0.363 cycle_A: 0.589 idt_A: 0.233 D_B: 0.200 G_B: 1.211 cycle_B: 0.523 idt_B: 0.226 \n",
      "(epoch: 9, iters: 832, time: 0.863, data: 0.000) D_A: 0.235 G_A: 0.256 cycle_A: 1.461 idt_A: 0.109 D_B: 0.109 G_B: 0.141 cycle_B: 0.265 idt_B: 0.674 \n",
      "(epoch: 9, iters: 932, time: 0.722, data: 0.001) D_A: 0.045 G_A: 0.563 cycle_A: 0.570 idt_A: 0.142 D_B: 0.325 G_B: 0.531 cycle_B: 0.296 idt_B: 0.234 \n",
      "(epoch: 9, iters: 1032, time: 0.696, data: 0.001) D_A: 0.381 G_A: 0.907 cycle_A: 1.252 idt_A: 0.882 D_B: 0.096 G_B: 0.852 cycle_B: 1.660 idt_B: 0.446 \n",
      "End of epoch 9 / 200 \t Time Taken: 435 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 10, iters: 36, time: 0.705, data: 0.000) D_A: 0.203 G_A: 0.194 cycle_A: 1.400 idt_A: 0.119 D_B: 0.055 G_B: 0.370 cycle_B: 0.296 idt_B: 0.640 \n",
      "(epoch: 10, iters: 136, time: 1.007, data: 0.000) D_A: 0.235 G_A: 0.192 cycle_A: 1.098 idt_A: 0.248 D_B: 0.077 G_B: 0.370 cycle_B: 0.546 idt_B: 0.359 \n",
      "saving the latest model (epoch 10, total_iters 10000)\n",
      "(epoch: 10, iters: 236, time: 0.694, data: 0.000) D_A: 0.084 G_A: 0.511 cycle_A: 1.173 idt_A: 0.208 D_B: 0.222 G_B: 0.077 cycle_B: 0.417 idt_B: 0.535 \n",
      "(epoch: 10, iters: 336, time: 0.779, data: 0.000) D_A: 0.135 G_A: 0.798 cycle_A: 0.970 idt_A: 0.990 D_B: 0.040 G_B: 0.318 cycle_B: 1.860 idt_B: 0.477 \n",
      "(epoch: 10, iters: 436, time: 0.693, data: 0.000) D_A: 0.258 G_A: 0.392 cycle_A: 1.088 idt_A: 0.136 D_B: 0.204 G_B: 0.267 cycle_B: 0.332 idt_B: 0.465 \n",
      "(epoch: 10, iters: 536, time: 0.865, data: 0.001) D_A: 0.352 G_A: 0.310 cycle_A: 1.080 idt_A: 0.115 D_B: 0.135 G_B: 0.366 cycle_B: 0.297 idt_B: 0.336 \n",
      "(epoch: 10, iters: 636, time: 0.698, data: 0.000) D_A: 0.143 G_A: 0.358 cycle_A: 0.885 idt_A: 0.164 D_B: 0.293 G_B: 0.133 cycle_B: 0.393 idt_B: 0.665 \n",
      "(epoch: 10, iters: 736, time: 0.696, data: 0.001) D_A: 0.139 G_A: 0.489 cycle_A: 1.056 idt_A: 0.231 D_B: 0.223 G_B: 0.516 cycle_B: 0.314 idt_B: 0.355 \n",
      "(epoch: 10, iters: 836, time: 0.693, data: 0.001) D_A: 0.196 G_A: 0.720 cycle_A: 0.863 idt_A: 0.132 D_B: 0.067 G_B: 0.097 cycle_B: 0.291 idt_B: 0.321 \n",
      "(epoch: 10, iters: 936, time: 0.840, data: 0.000) D_A: 0.099 G_A: 0.754 cycle_A: 1.045 idt_A: 0.110 D_B: 0.041 G_B: 0.114 cycle_B: 0.261 idt_B: 0.456 \n",
      "(epoch: 10, iters: 1036, time: 0.715, data: 0.001) D_A: 0.208 G_A: 0.466 cycle_A: 0.895 idt_A: 0.119 D_B: 0.197 G_B: 0.253 cycle_B: 0.260 idt_B: 0.356 \n",
      "saving the model at the end of epoch 10, iters 10960\n",
      "End of epoch 10 / 200 \t Time Taken: 434 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 11, iters: 40, time: 0.688, data: 0.001) D_A: 0.102 G_A: 0.336 cycle_A: 1.208 idt_A: 0.143 D_B: 0.245 G_B: 0.139 cycle_B: 0.303 idt_B: 0.496 \n",
      "(epoch: 11, iters: 140, time: 0.694, data: 0.000) D_A: 0.178 G_A: 0.851 cycle_A: 1.023 idt_A: 0.163 D_B: 0.047 G_B: 0.122 cycle_B: 0.400 idt_B: 0.822 \n",
      "(epoch: 11, iters: 240, time: 0.931, data: 0.001) D_A: 0.080 G_A: 0.508 cycle_A: 0.843 idt_A: 0.162 D_B: 0.059 G_B: 0.744 cycle_B: 0.335 idt_B: 0.353 \n",
      "(epoch: 11, iters: 340, time: 0.694, data: 0.001) D_A: 0.304 G_A: 0.132 cycle_A: 0.903 idt_A: 0.114 D_B: 0.162 G_B: 0.639 cycle_B: 0.243 idt_B: 0.355 \n",
      "(epoch: 11, iters: 440, time: 0.751, data: 0.000) D_A: 0.126 G_A: 0.453 cycle_A: 1.083 idt_A: 0.105 D_B: 0.145 G_B: 0.327 cycle_B: 0.229 idt_B: 0.495 \n",
      "(epoch: 11, iters: 540, time: 0.693, data: 0.000) D_A: 0.030 G_A: 0.754 cycle_A: 0.758 idt_A: 0.185 D_B: 0.117 G_B: 0.274 cycle_B: 0.433 idt_B: 0.304 \n",
      "(epoch: 11, iters: 640, time: 0.784, data: 0.001) D_A: 0.070 G_A: 0.522 cycle_A: 1.095 idt_A: 0.132 D_B: 0.057 G_B: 0.780 cycle_B: 0.263 idt_B: 0.390 \n",
      "(epoch: 11, iters: 740, time: 0.697, data: 0.000) D_A: 0.318 G_A: 1.566 cycle_A: 0.991 idt_A: 0.193 D_B: 0.143 G_B: 0.743 cycle_B: 0.510 idt_B: 0.376 \n",
      "(epoch: 11, iters: 840, time: 0.691, data: 0.001) D_A: 0.221 G_A: 0.367 cycle_A: 1.375 idt_A: 0.144 D_B: 0.154 G_B: 0.458 cycle_B: 0.396 idt_B: 0.710 \n",
      "(epoch: 11, iters: 940, time: 0.693, data: 0.000) D_A: 0.237 G_A: 0.484 cycle_A: 0.551 idt_A: 0.193 D_B: 0.223 G_B: 0.216 cycle_B: 0.264 idt_B: 0.159 \n",
      "(epoch: 11, iters: 1040, time: 1.058, data: 0.000) D_A: 0.110 G_A: 0.866 cycle_A: 0.962 idt_A: 0.154 D_B: 0.072 G_B: 0.429 cycle_B: 0.346 idt_B: 0.404 \n",
      "End of epoch 11 / 200 \t Time Taken: 432 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 12, iters: 44, time: 0.692, data: 0.001) D_A: 0.096 G_A: 0.800 cycle_A: 1.011 idt_A: 0.123 D_B: 0.149 G_B: 0.507 cycle_B: 0.256 idt_B: 0.448 \n",
      "(epoch: 12, iters: 144, time: 0.698, data: 0.000) D_A: 0.027 G_A: 0.869 cycle_A: 0.928 idt_A: 0.219 D_B: 0.148 G_B: 0.954 cycle_B: 0.550 idt_B: 0.410 \n",
      "(epoch: 12, iters: 244, time: 0.695, data: 0.001) D_A: 0.259 G_A: 0.849 cycle_A: 1.160 idt_A: 0.118 D_B: 0.024 G_B: 0.392 cycle_B: 0.280 idt_B: 0.432 \n",
      "(epoch: 12, iters: 344, time: 1.070, data: 0.000) D_A: 0.086 G_A: 0.507 cycle_A: 1.264 idt_A: 0.115 D_B: 0.046 G_B: 0.494 cycle_B: 0.340 idt_B: 0.543 \n",
      "(epoch: 12, iters: 444, time: 0.722, data: 0.001) D_A: 0.276 G_A: 0.455 cycle_A: 1.297 idt_A: 0.134 D_B: 0.078 G_B: 0.202 cycle_B: 0.426 idt_B: 0.618 \n",
      "(epoch: 12, iters: 544, time: 0.722, data: 0.000) D_A: 0.055 G_A: 0.473 cycle_A: 1.106 idt_A: 0.087 D_B: 0.121 G_B: 0.517 cycle_B: 0.203 idt_B: 0.511 \n",
      "(epoch: 12, iters: 644, time: 0.729, data: 0.000) D_A: 0.147 G_A: 0.240 cycle_A: 2.017 idt_A: 0.173 D_B: 0.105 G_B: 0.955 cycle_B: 0.371 idt_B: 0.772 \n",
      "(epoch: 12, iters: 744, time: 0.880, data: 0.000) D_A: 0.187 G_A: 0.521 cycle_A: 0.913 idt_A: 0.129 D_B: 0.237 G_B: 0.217 cycle_B: 0.368 idt_B: 0.714 \n",
      "(epoch: 12, iters: 844, time: 0.717, data: 0.001) D_A: 0.026 G_A: 0.383 cycle_A: 0.929 idt_A: 0.159 D_B: 0.036 G_B: 0.124 cycle_B: 0.337 idt_B: 0.358 \n",
      "(epoch: 12, iters: 944, time: 0.760, data: 0.000) D_A: 0.030 G_A: 0.749 cycle_A: 1.241 idt_A: 0.149 D_B: 0.210 G_B: 0.229 cycle_B: 0.391 idt_B: 0.520 \n",
      "(epoch: 12, iters: 1044, time: 0.714, data: 0.001) D_A: 0.276 G_A: 0.118 cycle_A: 1.289 idt_A: 0.117 D_B: 0.144 G_B: 0.083 cycle_B: 0.247 idt_B: 0.590 \n",
      "End of epoch 12 / 200 \t Time Taken: 441 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 13, iters: 48, time: 1.045, data: 0.000) D_A: 0.114 G_A: 0.236 cycle_A: 0.932 idt_A: 0.118 D_B: 0.033 G_B: 0.207 cycle_B: 0.267 idt_B: 0.351 \n",
      "(epoch: 13, iters: 148, time: 0.712, data: 0.000) D_A: 0.070 G_A: 0.701 cycle_A: 0.975 idt_A: 0.099 D_B: 0.226 G_B: 0.170 cycle_B: 0.236 idt_B: 0.455 \n",
      "(epoch: 13, iters: 248, time: 0.713, data: 0.000) D_A: 0.172 G_A: 1.183 cycle_A: 1.660 idt_A: 0.199 D_B: 0.067 G_B: 0.592 cycle_B: 0.825 idt_B: 0.533 \n",
      "(epoch: 13, iters: 348, time: 0.714, data: 0.001) D_A: 0.195 G_A: 0.581 cycle_A: 0.517 idt_A: 0.143 D_B: 0.062 G_B: 0.241 cycle_B: 0.353 idt_B: 0.332 \n",
      "(epoch: 13, iters: 448, time: 0.889, data: 0.001) D_A: 0.067 G_A: 0.711 cycle_A: 1.833 idt_A: 0.162 D_B: 0.085 G_B: 0.515 cycle_B: 0.395 idt_B: 0.725 \n",
      "(epoch: 13, iters: 548, time: 0.713, data: 0.001) D_A: 0.049 G_A: 0.307 cycle_A: 0.570 idt_A: 0.155 D_B: 0.202 G_B: 0.673 cycle_B: 0.320 idt_B: 0.252 \n",
      "(epoch: 13, iters: 648, time: 0.714, data: 0.001) D_A: 0.202 G_A: 0.148 cycle_A: 0.858 idt_A: 0.113 D_B: 0.066 G_B: 0.197 cycle_B: 0.276 idt_B: 0.351 \n",
      "(epoch: 13, iters: 748, time: 0.710, data: 0.000) D_A: 0.151 G_A: 0.842 cycle_A: 0.741 idt_A: 0.143 D_B: 0.114 G_B: 0.101 cycle_B: 0.322 idt_B: 0.276 \n",
      "(epoch: 13, iters: 848, time: 1.052, data: 0.001) D_A: 0.148 G_A: 0.677 cycle_A: 1.097 idt_A: 0.111 D_B: 0.272 G_B: 0.137 cycle_B: 0.254 idt_B: 0.376 \n",
      "(epoch: 13, iters: 948, time: 0.714, data: 0.001) D_A: 0.219 G_A: 0.165 cycle_A: 1.043 idt_A: 0.082 D_B: 0.094 G_B: 1.027 cycle_B: 0.185 idt_B: 0.470 \n",
      "(epoch: 13, iters: 1048, time: 0.715, data: 0.001) D_A: 0.066 G_A: 0.547 cycle_A: 1.018 idt_A: 0.509 D_B: 0.169 G_B: 0.691 cycle_B: 1.256 idt_B: 0.380 \n",
      "End of epoch 13 / 200 \t Time Taken: 442 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 14, iters: 52, time: 0.715, data: 0.000) D_A: 0.156 G_A: 0.478 cycle_A: 0.758 idt_A: 0.121 D_B: 0.193 G_B: 0.522 cycle_B: 0.340 idt_B: 0.297 \n",
      "(epoch: 14, iters: 152, time: 1.069, data: 0.001) D_A: 0.083 G_A: 0.227 cycle_A: 1.227 idt_A: 0.153 D_B: 0.160 G_B: 0.350 cycle_B: 0.374 idt_B: 0.492 \n",
      "(epoch: 14, iters: 252, time: 0.715, data: 0.000) D_A: 0.222 G_A: 0.308 cycle_A: 0.803 idt_A: 0.113 D_B: 0.163 G_B: 0.421 cycle_B: 0.279 idt_B: 0.316 \n",
      "(epoch: 14, iters: 352, time: 0.718, data: 0.000) D_A: 0.101 G_A: 0.664 cycle_A: 0.587 idt_A: 0.132 D_B: 0.164 G_B: 0.232 cycle_B: 0.279 idt_B: 0.265 \n",
      "(epoch: 14, iters: 452, time: 0.726, data: 0.001) D_A: 0.148 G_A: 0.506 cycle_A: 1.225 idt_A: 0.172 D_B: 0.164 G_B: 0.204 cycle_B: 0.464 idt_B: 0.539 \n",
      "(epoch: 14, iters: 552, time: 0.861, data: 0.001) D_A: 0.110 G_A: 0.467 cycle_A: 1.179 idt_A: 0.140 D_B: 0.095 G_B: 0.633 cycle_B: 0.951 idt_B: 0.478 \n",
      "(epoch: 14, iters: 652, time: 0.715, data: 0.000) D_A: 0.156 G_A: 0.412 cycle_A: 0.922 idt_A: 0.115 D_B: 0.072 G_B: 0.108 cycle_B: 0.276 idt_B: 0.337 \n",
      "(epoch: 14, iters: 752, time: 0.714, data: 0.001) D_A: 0.249 G_A: 0.216 cycle_A: 0.895 idt_A: 0.224 D_B: 0.106 G_B: 0.470 cycle_B: 0.500 idt_B: 0.357 \n",
      "saving the latest model (epoch 14, total_iters 15000)\n",
      "(epoch: 14, iters: 852, time: 0.714, data: 0.001) D_A: 0.110 G_A: 0.412 cycle_A: 0.970 idt_A: 0.175 D_B: 0.104 G_B: 0.410 cycle_B: 0.465 idt_B: 0.366 \n",
      "(epoch: 14, iters: 952, time: 0.957, data: 0.000) D_A: 0.247 G_A: 0.154 cycle_A: 1.002 idt_A: 0.181 D_B: 0.206 G_B: 0.336 cycle_B: 0.464 idt_B: 0.381 \n",
      "(epoch: 14, iters: 1052, time: 0.720, data: 0.001) D_A: 0.061 G_A: 0.767 cycle_A: 0.991 idt_A: 0.129 D_B: 0.156 G_B: 0.234 cycle_B: 0.376 idt_B: 0.359 \n",
      "End of epoch 14 / 200 \t Time Taken: 445 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 15, iters: 56, time: 0.710, data: 0.001) D_A: 0.068 G_A: 0.408 cycle_A: 1.267 idt_A: 0.194 D_B: 0.088 G_B: 0.878 cycle_B: 0.472 idt_B: 0.364 \n",
      "(epoch: 15, iters: 156, time: 0.712, data: 0.001) D_A: 0.252 G_A: 0.435 cycle_A: 1.036 idt_A: 0.135 D_B: 0.060 G_B: 0.186 cycle_B: 0.307 idt_B: 0.414 \n",
      "(epoch: 15, iters: 256, time: 1.059, data: 0.000) D_A: 0.381 G_A: 0.566 cycle_A: 1.028 idt_A: 0.195 D_B: 0.267 G_B: 0.578 cycle_B: 0.501 idt_B: 0.365 \n",
      "(epoch: 15, iters: 356, time: 0.696, data: 0.001) D_A: 0.200 G_A: 0.679 cycle_A: 0.862 idt_A: 0.208 D_B: 0.170 G_B: 0.254 cycle_B: 0.453 idt_B: 0.343 \n",
      "(epoch: 15, iters: 456, time: 0.713, data: 0.001) D_A: 0.185 G_A: 0.448 cycle_A: 0.987 idt_A: 0.168 D_B: 0.384 G_B: 0.463 cycle_B: 0.481 idt_B: 0.364 \n",
      "(epoch: 15, iters: 556, time: 0.690, data: 0.000) D_A: 0.028 G_A: 0.852 cycle_A: 0.775 idt_A: 0.138 D_B: 0.141 G_B: 0.711 cycle_B: 0.401 idt_B: 0.324 \n",
      "(epoch: 15, iters: 656, time: 1.115, data: 0.001) D_A: 0.184 G_A: 0.524 cycle_A: 0.856 idt_A: 0.220 D_B: 0.136 G_B: 0.424 cycle_B: 0.821 idt_B: 0.328 \n",
      "(epoch: 15, iters: 756, time: 0.695, data: 0.001) D_A: 0.191 G_A: 0.340 cycle_A: 0.829 idt_A: 0.167 D_B: 0.190 G_B: 0.250 cycle_B: 0.418 idt_B: 0.386 \n",
      "(epoch: 15, iters: 856, time: 0.724, data: 0.001) D_A: 0.254 G_A: 0.319 cycle_A: 1.286 idt_A: 0.176 D_B: 0.215 G_B: 0.954 cycle_B: 0.470 idt_B: 0.515 \n",
      "(epoch: 15, iters: 956, time: 0.696, data: 0.001) D_A: 0.169 G_A: 0.248 cycle_A: 0.780 idt_A: 0.104 D_B: 0.151 G_B: 0.177 cycle_B: 0.233 idt_B: 0.300 \n",
      "(epoch: 15, iters: 1056, time: 0.917, data: 0.001) D_A: 0.155 G_A: 0.474 cycle_A: 0.922 idt_A: 0.157 D_B: 0.053 G_B: 0.323 cycle_B: 0.407 idt_B: 0.423 \n",
      "saving the model at the end of epoch 15, iters 16440\n",
      "End of epoch 15 / 200 \t Time Taken: 436 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 16, iters: 60, time: 0.695, data: 0.001) D_A: 0.087 G_A: 0.321 cycle_A: 0.948 idt_A: 0.119 D_B: 0.241 G_B: 0.237 cycle_B: 0.311 idt_B: 0.394 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32mD:\\cyclegan\\train.py:51\u001B[0m\n\u001B[0;32m     49\u001B[0m total_iters \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m opt\u001B[38;5;241m.\u001B[39mbatch_size\n\u001B[0;32m     50\u001B[0m epoch_iter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m opt\u001B[38;5;241m.\u001B[39mbatch_size\n\u001B[1;32m---> 51\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m         \u001B[38;5;66;03m# unpack data from dataset and apply preprocessing\u001B[39;00m\n\u001B[0;32m     52\u001B[0m model\u001B[38;5;241m.\u001B[39moptimize_parameters()   \u001B[38;5;66;03m# calculate loss functions, get gradients, update network weights\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total_iters \u001B[38;5;241m%\u001B[39m opt\u001B[38;5;241m.\u001B[39mdisplay_freq \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:   \u001B[38;5;66;03m# display images on visdom and save images to a HTML file\u001B[39;00m\n",
      "File \u001B[1;32mD:\\cyclegan\\models\\cycle_gan_model.py:108\u001B[0m, in \u001B[0;36mCycleGANModel.set_input\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;124;03m\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03mParameters:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03mThe option 'direction' can be used to swap domain A and domain B.\u001B[39;00m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    107\u001B[0m AtoB \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopt\u001B[38;5;241m.\u001B[39mdirection \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAtoB\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreal_A \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mA\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mAtoB\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreal_B \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m AtoB \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA_paths\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m AtoB \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mB_paths\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%run  train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CycleGAN",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}